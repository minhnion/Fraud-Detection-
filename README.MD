# Real-Time Fraud Detection System

This project is a **real-time credit card fraud detection system** built on a modern Big Data architecture, leveraging **Apache Kafka, Apache Flink, and MinIO**.  
The system is capable of processing a continuous stream of transactions, applying a **Machine Learning model** to detect fraud in real-time, while also persisting all raw data for future analysis and model retraining.

---

## âœ¨ Key Features

- **Stream Processing**: Handles thousands of transactions per second in real-time.  
- **Fraud Detection with ML**: Integrates a **Scikit-learn Machine Learning model (Random Forest)** for more accurate fraud classification compared to static rules.  
- **Persistent Storage (Data Lake)**: Stores all raw transactions in **MinIO (S3-compatible)** for safe long-term storage, retraining, or batch analytics.  
- **Microservices & Containerization**: Each component is containerized with **Docker** and orchestrated with **Docker Compose**, simplifying deployment and scalability.  
- **Monitoring & Alerts**: Provides a dedicated alert channel for transactions flagged as potentially fraudulent.  

---

## ğŸ›ï¸ System Architecture

The system is composed of the following core components:

- **Apache Kafka**: Acts as the *central nervous system* (message bus).  
  All transactions from the data source are pushed into Kafka, ensuring a fault-tolerant, durable, and decoupled data stream between producers and consumers.

- **Apache Flink**: Serves as the *real-time processing brain*.  
  Flink consumes data from Kafka and performs two parallel tasks:  
  - **Inference Stream**: Applies the trained ML model to predict fraud for each transaction.  
  - **Storage Stream**: Writes a copy of all raw transactions to the Data Lake.  

- **MinIO**: An **S3-compatible object storage system**, serving as the Data Lake to store raw transaction data and trained ML models.  

- **Scikit-learn**: Used to train the fraud detection model (**Random Forest**) from historical data.  
  The trained model is later loaded by Flink for real-time inference.  

- **Docker & Docker Compose**: Package all components (Kafka, Flink, MinIO) into isolated containers and define their interactions, making environment setup and scaling straightforward.  

---

## ğŸŒŠ Pipeline Workflow

The following diagram illustrates the flow of data through the system,  
including both the **offline training pipeline** and the **real-time prediction pipeline**.

```text
Offline Training (one-time)
---------------------------
[CSV Dataset] 
      â”‚
      â–¼
train_sklearn_model.py
      â”‚
      â–¼
 [Model.pkl]
      â”‚
      â–¼
[MinIO Data Lake]


Online Prediction (real-time)
-----------------------------
[Producer] ---> (Kafka Topic: transactions) ---> [Flink Job: ml_inference_job.py]
                                                       â”‚
                                                       â”‚ (load trained model from MinIO at startup)
                                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                                                              â”‚
                Fraud Prediction                                                 Store Raw Data
                        â”‚                                                              â”‚
                        â–¼                                                              â–¼
        (Kafka Topic: fraud_alerts)                                      [Flink Job: kafka_to_minio_job.py]
                        â”‚                                                              â”‚
                        â–¼                                                              â–¼
             [Consumer / Alerting System]                                   [MinIO Data Lake]

```


## ğŸš€ How to Run the Project

### Requirements
- Docker and Docker Compose
- Python 3.8+ and pip
- Python virtual environment (recommended)

---

### Step 1: Initial Setup

Clone the repository:
```bash
git clone <your-repo-url>
cd fraud-detection
```
Create and activate a virtual environment:
```bash
python3 -m venv .venv
source .venv/bin/activate
```
Install Python dependencies:
```bash
pip install -r requirements.txt
```

### BÆ°á»›c 2: Huáº¥n luyá»‡n MÃ´ hÃ¬nh Machine Learning

Cháº¡y script Ä‘á»ƒ huáº¥n luyá»‡n vÃ  lÆ°u mÃ´ hÃ¬nh.  
MÃ´ hÃ¬nh nÃ y sáº½ Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng file `.pkl` vÃ  Ä‘Æ°á»£c Flink job táº£i lÃªn tá»« MinIO Ä‘á»ƒ dá»± Ä‘oÃ¡n.

```bash
python train_sklearn_model.py
```
Sau khi cháº¡y, báº¡n sáº½ tháº¥y file sklearn_fraud_model.pkl Ä‘Æ°á»£c táº¡o trong thÆ° má»¥c data/model/.

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng Háº¡ táº§ng Big Data

Sá»­ dá»¥ng **Docker Compose** Ä‘á»ƒ xÃ¢y dá»±ng Flink image vÃ  khá»Ÿi Ä‘á»™ng toÃ n bá»™ cÃ¡c dá»‹ch vá»¥ (Kafka, Flink, MinIO):

```bash
docker-compose up --build -d
```
Trong Ä‘Ã³:
TÃ¹y chá»n --build Ä‘áº£m báº£o Flink image Ä‘Æ°á»£c rebuild vá»›i cÃ¡c thÆ° viá»‡n ML Ä‘Ã£ khai bÃ¡o trong Dockerfile. Tuá»³ chá»n -d cháº¡y cÃ¡c container á»Ÿ cháº¿ Ä‘á»™ ná»n (detached mode).

### BÆ°á»›c 4: Chuáº©n bá»‹ MinIO vÃ  Kafka Topics

#### 1. Táº¡o MinIO Bucket
- Má»Ÿ trÃ¬nh duyá»‡t vÃ  truy cáº­p: [http://localhost:9001](http://localhost:9001)  
- ÄÄƒng nháº­p vá»›i tÃ i khoáº£n máº·c Ä‘á»‹nh:
  - **User:** `minioadmin`
  - **Password:** `minioadmin`  
- Nháº¥n **"Create Bucket"** vÃ  táº¡o má»™t bucket má»›i tÃªn lÃ  **`datalake`**

#### 2. (TÃ¹y chá»n nhÆ°ng khuyáº¿n khÃ­ch) Táº¡o Kafka Topics
Cháº¡y cÃ¡c lá»‡nh sau Ä‘á»ƒ táº¡o sáºµn topic, trÃ¡nh lá»—i *"Unknown Topic"* khi Flink job cháº¡y:

```bash
docker exec -it kafka kafka-topics --create --topic transactions --bootstrap-server kafka:29092
docker exec -it kafka kafka-topics --create --topic fraud_alerts --bootstrap-server kafka:29092
```

### BÆ°á»›c 5: Cháº¡y Pipeline HoÃ n chá»‰nh

Báº¡n sáº½ cáº§n má»Ÿ **3 cá»­a sá»• terminal riÃªng biá»‡t** Ä‘á»ƒ khá»Ÿi Ä‘á»™ng toÃ n bá»™ pipeline.

---

#### ğŸ–¥ï¸ Terminal 1 â€“ Submit Flink Jobs
Ná»™p cáº£ hai Flink job Ä‘á»ƒ báº¯t Ä‘áº§u xá»­ lÃ½ dá»¯ liá»‡u:

```bash
# Job 1: DÃ¹ng ML Ä‘á»ƒ phÃ¡t hiá»‡n gian láº­n
docker exec -it flink-jobmanager /opt/flink/bin/flink run -py /app/flink_jobs/ml_inference_job.py

# Job 2: LÆ°u trá»¯ dá»¯ liá»‡u thÃ´ vÃ o MinIO
docker exec -it flink-jobmanager /opt/flink/bin/flink run -py /app/flink_jobs/kafka_to_minio_job.py
```
ğŸ“Œ Kiá»ƒm tra: Truy cáº­p Flink UI táº¡i http://localhost:8081

### ğŸ–¥ï¸ Terminal 2 â€“ Cháº¡y Consumer Ä‘á»ƒ nháº­n Cáº£nh bÃ¡o

Consumer script sáº½ láº¯ng nghe vÃ  in ra cÃ¡c cáº£nh bÃ¡o gian láº­n mÃ  Flink phÃ¡t hiá»‡n:

```bash
python src/consumer/consumer.py
```
(Terminal nÃ y sáº½ luÃ´n chá» Ä‘á»ƒ hiá»ƒn thá»‹ tin nháº¯n tá»« Kafka.)

### ğŸ–¥ï¸ Terminal 3 â€“ Cháº¡y Producer Ä‘á»ƒ gá»­i Dá»¯ liá»‡
Producer script báº¯t Ä‘áº§u phÃ¡t dá»¯ liá»‡u giao dá»‹ch vÃ o Kafka:
```bash
python src/producer/producer.py
```
ğŸ‘‰ ÄÃ¢y lÃ  bÆ°á»›c cuá»‘i cÃ¹ng Ä‘á»ƒ kÃ­ch hoáº¡t toÃ n bá»™ luá»“ng dá»¯ liá»‡u.

### BÆ°á»›c 6: Quan sÃ¡t Káº¿t quáº£

- **Terminal 2 (Consumer):**  
  Báº¡n sáº½ tháº¥y cÃ¡c báº£n ghi giao dá»‹ch Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c in ra mÃ n hÃ¬nh má»—i khi mÃ´ hÃ¬nh ML dá»± Ä‘oÃ¡n má»™t giao dá»‹ch lÃ  **gian láº­n**.

- **MinIO UI ([http://localhost:9001](http://localhost:9001)):**  
  Sau vÃ i phÃºt, truy cáº­p bucket **`datalake`** â†’ báº¡n sáº½ tháº¥y thÆ° má»¥c **`raw/transactions/`** Ä‘Æ°á»£c táº¡o, trong Ä‘Ã³ chá»©a cÃ¡c tá»‡p dá»¯ liá»‡u giao dá»‹ch thÃ´ mÃ  Flink Ä‘Ã£ ghi vÃ o.

---

### BÆ°á»›c 7: Dá»n dáº¹p

Äá»ƒ dá»«ng vÃ  xÃ³a toÃ n bá»™ cÃ¡c container Ä‘ang cháº¡y, dÃ¹ng lá»‡nh:

```bash
docker-compose down
```
